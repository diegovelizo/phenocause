---
output:
  github_document:
    html_preview: false
    toc: true
    toc_depth: 3
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
collapse = TRUE,
comment = "#>",
fig.path = "man/figures/",
fig.width = 8,
fig.height = 6,
message = FALSE,
warning = FALSE
)
```

# phenocause: a package for simulating complex phenotypes  
  
A central challenge in genetic epidemiology is to disentangle the genetic basis of a complex trait from the effects of environmental and social factors, especially when those factors are correlated with genetic ancestry or the true polygenic score. `phenocause` is a simulation toolkit designed to address this problem by allowing researchers to simulate phenotypes where the contributions of genetics, population structure, and different types of confounding can be explicitly and precisely controlled. The package implements a two-step workflow: first, sampling a set of causal variants to define a trait's genetic architecture, and second, simulating a final phenotype based on those variants under a variety of user-specified confounding models. This package includes example genotype data and metadata to allow users to follow the tutorial.  

## motivation and a note on confounding  
  
The development of `phenocause` was driven by a specific theoretical motivation regarding the nature of confounding in genetic studies. This framework relies on a critical distinction between two types of genetic relationship matrices (GRMs): "shallow GRMs", which are designed to quantify only recent relatedness (e.g., kinship coefficients from IBD-based methods), and "deep GRMs", which are derived from coalescent theory and aim to capture the full, complete genetic covariance resulting from both recent and distant shared ancestry.  

The central hypothesis motivating this package is that when a mixed model includes a well-specified "deep GRM", it fully accounts for the genetic covariance structure among all individuals. Consequently, any remaining inflation in test statistics must be due to external factors, such as environmental confounding correlated with ancestry, rather than residual "genetic confounding".  

Under this framework, principal components (PCs) become irrelevant for controlling *genetic* effects, and their utility is restricted to serving as proxies for these unmodeled environmental confounders. `phenocause` was therefore developed as a companion toolkit to generate the precise data structures needed to benchmark analytical methods and test this hypothesis empirically.  
  
## the simulation framework: theoretical models

`phenocause` generates phenotypes using an additive model. All confounding effects are optional and are added to a base genetic model. The quantitative liability for a trait ($Y$) is simulated as the sum of a genetic component ($g$), one or more optional confounding effects ($P_k$), and a random residual error ($E$).

The general model is:

$$Y = \mu + g + P_{cc} + P_{gc} + P_{ac} + E$$

Where:

* **$Y$** is the final quantitative phenotype, which is treated as an unobserved liability for binary (case/control) traits.
* **$\mu$** is the baseline mean of the phenotype.
* **$g$** is the individual's true total genetic effect, or polygenic score, determined by their causal alleles.
* **$P_{cc}$** is the additive effect from a **c**ategorical **c**onfounder (e.g., socioeconomic status).
* **$P_{gc}$** is the additive effect from a quantitative confounder correlated with the **g**enetic component $g$.
* **$P_{ac}$** is the additive effect from a quantitative confounder correlated with a specific **a**ncestry **c**omponent $Q$.
* **$E$** is a normally-distributed residual error term, the variance of which is scaled to meet the target heritability.

### the genetic component ($g$)

The total genetic effect for an individual $i$ is the sum of their dosages at $M$ causal variants, weighted by each variant's effect size, $\beta_j$:

$$g_i = \sum_{j=1}^{M} g_{ij}\beta_{j}$$

The effect sizes are drawn from a normal distribution whose variance is dependent on the allele frequency $p_j$ of the variant, controlled by the parameter $\alpha$. This model is adopted from the LDAK software.

$$\beta_{j} \sim \mathcal{N}(0, [2p_{j}(1-p_{j})]^{\alpha})$$

The $\alpha$ parameter models the relationship between allele frequency and the variance explained by a variant. A value of $\alpha = -1$, the default, corresponds to the standard GCTA model where all variants are assumed to contribute equally to heritability, regardless of their frequency. Other values can be used to model architectures where, for example, low-frequency variants have larger effects.

### the confounding components ($P$)  

`phenocause` can simulate three distinct types of confounding.  

#### categorical confounder ($P_{cc}$)  

This models an observable categorical variable, $X_{cc}$, that influences the phenotype. The total effect is $P_{cc} = X_{cc}\beta_{cc}$. The simulation distinguishes between two scales:  

* **Nominal Scale**: When the input is a `character` vector (e.g., country of birth, population label), each category is assigned an independent, random effect.  

* **Ordinal Scale**: When the input is an ordered `factor` (e.g., educational attainment levels), the effects are simulated to be monotonically increasing across the factor levels.

The total variance explained by this component is controlled by the `categorical_confounder_variance` parameter.  

#### quantitative confounders ($P_{gc}$ and $P_{ac}$)  

The package can simulate two types of quantitative confounders that represent distinct causal scenarios. The distinction between these models is critical for designing simulation studies to test different hypotheses.  

The **ancestry-correlated confounder ($P_{ac}$)** is designed to simulate classic confounding by population structure, where an external socio-environmental factor ($X_{ac}$) is correlated with an ancestry component ($Q$). It helps answer questions about the robustness of GWAS methods to this common source of bias. The simulation is governed by four parameters:  

* `ancestry_component` ($Q$): The vector of ancestry values for each individual.  
* `ancestry_confounder_coeff` ($\gamma_{ac}$): The direct effect of the confounder, where $P_{ac} = X_{ac}\gamma_{ac}$.  
* `ancestry_confounder_cor` ($\rho_{ac}$): The target correlation between the ancestry component and the confounder, $Cor(Q, X_{ac})$.  
* `ancestry_confounder_rel_var` ($w_{ac}$): The target variance of the confounding effect relative to the genetic variance, $Var(P_{ac})/Var(g)$.  

In contrast, the **genetically-correlated confounder ($P_{gc}$)** is designed to simulate confounding from pleiotropy or a gene-environment correlation pathway, where a latent quantitative confounder ($X_{gc}$) is correlated with the true polygenic score ($g$) itself. It helps answer different questions, such as whether an association signal is driven by a direct genetic effect or an indirect one mediated by another genetically-influenced trait. The simulation is governed by three analogous parameters:  

* `genetic_confounder_coeff` ($\gamma_{gc}$): The direct linear effect of the confounder on the phenotype, where $P_{gc} = X_{gc}\gamma_{gc}$.  
* `genetic_confounder_cor` ($\rho_{gc}$): The target correlation between the genetic effect and the confounder, $Cor(g, X_{gc})$.  
* `genetic_confounder_rel_var` ($w_{gc}$): The target variance of the confounding effect *relative to* the genetic variance, $Var(P_{gc})/Var(g)$.  

### simulating binary traits  
  
For binary (case/control) outcomes, the quantitative trait $Y$ is treated as an unobserved liability. A threshold is applied to this liability distribution, and individuals with a liability exceeding the threshold are assigned "case" status. The threshold value is determined by the user-specified `prevalence` of the trait.  


## installation  

You can install `phenocause` from GitHub using the `devtools` package.  

```{r installation-example, eval=FALSE}
if (!requireNamespace("devtools", quietly = TRUE)) {
  install.packages("devtools")
}
devtools::install_github("diegovelizo/phenocause")
```
   
## the two-step simulation workflow  
  
Below we demonstrate how to use `phenocause` to simulate phenotype data. The pipeline consists of two main steps: first, running one of the `sample_causal_sites.*()` functions to sample causal sites from genotype data in the GDS (Genomic Data Structure) format (see "[working with the genomic data structure (gds) format](#working-with-the-genomic-data-structure-gds-format)"), and then passing the resulting genotypes to the `simulate_phenotype()` function.  

You can skip the explanations and proceed directly to the [step-by-step tutorial](#step-by-step-tutorial).  

## working with the genomic data structure (gds) format  

### why gds?  
The package is designed to operate on genotype data stored in GDS files.  
GDS files store the same information as a VCF file; however, the GDS format is optimized for high-performance computation and efficient access.  
Its primary advantages are:  

* **Efficiency:** Data is stored in on-disk chunks, allowing for much faster access to specific variants or samples compared to parsing a text-based VCF file.  

* **Memory Management:** The entire dataset does not need to be loaded into memory.  
Instead, a connection is opened to the file on disk, and only the requested data slices are read.  

* **On-Disk Filtering:** Filters (e.g., by minor allele count, missingness or sample ID) can be applied to the file connection itself, before loading the data, ensuring that subsequent operations only process the subset of data that meets the specified criteria.  

### obtaining gds files  

This package includes example GDS files that can be used to follow the tutorial.  
The paths on your system can be retrieved with the following command:  

```{r get-example-gds-paths}
gds_paths <- system.file(
  paste0("extdata/example.chr", 1:3, ".gds"), package = "phenocause"
)
```

To use your own data, you must first convert it from VCF format to GDS using the `seqVCF2GDS()` function from the `SeqArray` package.  

```{r vcf-to-gds-example, eval=FALSE}
# Define path to an existing VCF file and the desired output path for the GDS file
vcf_path <- "/path/to/your/data.vcf.gz" 
gds_path <- "/path/to/your/output.gds" 

# Convert the VCF to GDS. GT (genotype) is the minimum required format.
SeqArray::seqVCF2GDS(vcf_path, gds_path, fmt.import="GT", storage.option="LZMA_RA", parallel = 4)
```

### gds access demonstration (optional)

Once you have a GDS file, you can open a connection and perform various high-performance operations.  
Note: This is just a demonstration of how GDS files can be efficiently accessed.  
These operations are done under the hood by `phenocause`, so you don't need to deal with them.  

```{r seqarray-tour}
# Use the first example GDS file
gds_path <- gds_paths[1] 

# 1. Open a connection to the GDS file (does not load data into memory)
gds <- SeqArray::seqOpen(gds_path)

# 2. Get basic information
sample_ids <- SeqArray::seqGetData(gds, "sample.id")
variant_ids <- SeqArray::seqGetData(gds, "variant.id") # These are internal indices
cat("First 6 sample IDs:", head(sample_ids), "\n")

# 3. Apply filters without loading data
# Get per-variant statistics
mac <- SeqArray::seqAlleleCount(gds, minor = TRUE)
missingness <- SeqArray::seqMissing(gds, per.variant = TRUE)

# Create boolean vectors for filtering
mac_filter <- mac >= 20
missingness_filter <- missingness <= 0.02
final_filter <- mac_filter & missingness_filter

# Apply the filter to the GDS connection. 
# Subsequent operations will only use variants that pass this filter.
SeqArray::seqSetFilter(gds, variant.sel = final_filter)

# 4. Access data only from the filtered sites
# Get annotations for the filtered variants
sites_annotation <-  data.frame(
    chr = SeqArray::seqGetData(gds, "chromosome"),
    pos = SeqArray::seqGetData(gds, "position"),
    rsid = SeqArray::seqGetData(gds, "annotation/id")
)
cat("\nAnnotations for the first 6 variants passing filters:\n")
print(head(sites_annotation))

# Get genotype dosages for the filtered variants
# The altDosage function returns a matrix of samples x variants
genotype_data <- SeqVarTools::altDosage(gdsobj = gds, use.names = TRUE, parallel = 2)
cat("\nDimensions of genotype matrix from filtered sites (samples x variants):\n")
print(dim(genotype_data))

# 5. Close the GDS file connection
SeqArray::seqClose(gds)
```


## data included with `phenocause`  

The package includes three data objects to support the tutorial.  
They can be loaded with `data()`.  

### genetic data  

The example GDS files contain genotypes for 5,000 individuals across portions of chromosomes 20, 21, and 22.  
The data was simulated using `msprime` with a demographic model of Latin American and reference continental populations resembling the 1000 Genomes Project populations, to provide a realistic background of population structure and admixture.  

### metadata (`phenocause.metadata`)  

This data frame contains sample-level information for the 5,000 simulated individuals, including population labels and the first 20 principal components calculated from the genetic data.  

```{r load-metadata}
data(phenocause.metadata)
cat("Metadata for the first 6 samples:\n")
print(head(phenocause.metadata[, 1:8]))
```

### ldak weights (`phenocause.ldak_weights`)  

This is a list of data frames containing pre-computed Linkage Disequilibrium (LD) weights from the LDAK software.  
Each data frame corresponds to a chromosome in the example dataset.  
These weights measure the extent of local LD surrounding each variant and can be used to simulate genetic architectures where causal variant probability is related to LD patterns.  

```{r load-ldak-weights}
data(phenocause.ldak_weights)
cat("LDAK weights for the first 6 variants on chromosome 1:\n")
# The data object is a list named by chromosome
print(head(phenocause.ldak_weights[["chr1"]]))
```



## step-by-step tutorial  

This tutorial demonstrates the two-step workflow for simulating phenotypes with `phenocause`.  
First, we select causal variants to define the genetic architecture.  
Second, we use those variants to simulate a phenotype with specified genetic and confounding effects.  

Before starting, we load the required libraries and get the paths to the example GDS files included with the package.  

```{r load-libraries}
# phenocause and data manipulation
library(phenocause)
library(dplyr)
library(glue)
library(data.table)

# Plotting
library(ggplot2)
library(patchwork)

# GDS and GWAS
library(SeqArray)
library(SeqVarTools)
library(GENESIS)
library(Biobase)
```

```{r get-gds-paths}
gds_paths <- system.file(
  paste0("extdata/example.chr", 1:3, ".gds"), package = "phenocause"
)
```

### part 1: defining the genetic architecture  

The first step is to define the degree of polygenicity and how the causal sites are distributed along the genome.  
For instance, we can sample causal sites with uniform probability, oversample them in high- or low-LD regions, or enrich them based on any user-defined genomic feature.  
The package provides several wrapper functions, `sample_causal_sites.*()`, for these different modes.  

#### example 1.1: uniform sampling (basic model)  

The simplest model of genetic architecture is one where every variant has an equal *a priori* probability of being causal.  
This is achieved by sampling with uniform probability using the `sample_causal_sites.uniform()` function.  
We will sample 1,000 sites and use this set for the phenotype simulation examples in Part 2.  

```{r uniform-sampling}
set.seed(123)
n_causal_sites <- 1000 

uniform_sites <- phenocause::sample_causal_sites.uniform(
  gds_paths = gds_paths,
  n_causal_sites = n_causal_sites,
  n_threads = 2
)

# The output is a list containing two data frames. We will use 'causal_genotypes'
# as the primary input for the phenotype simulation function.
causal_geno_df <- uniform_sites$causal_genotypes
cat("Dimensions of the sampled causal genotype matrix:\n")
print(dim(causal_geno_df))
```

#### example 1.2: weighted sampling (ldak weights)  

Complex trait genetics are shaped by evolutionary forces like negative selection, which may cause causal variants to be preferentially located in certain regions (e.g., regions of lower linkage disequilibrium).  
Using weights derived from local LD, such as those from the LDAK software, allows us to simulate such architectures.  
The `sample_causal_sites.ldak()` function uses these weights and a `weights_power` parameter to fine-tune the enrichment.  
For example, `weights_power = -0.25` up-weights variants with smaller LD weights (raw LDAK weights are ~1 in low LD, ~0 in high LD), thus enriching for causal variants in regions of higher LD.  

```{r ldak-sampling}
# Load the example LDAK weights data shipped with the package
data(phenocause.ldak_weights)
temp_dir <- tempdir()

# Create temporary 2-column LDAK weight files (no header) to demonstrate
temp_ldak_paths <- sapply(names(phenocause.ldak_weights), function(chr_name) {
  file_path <- file.path(temp_dir, paste0(chr_name, ".weights.short"))
  data.table::fwrite(
    phenocause.ldak_weights[[chr_name]],
    file = file_path, sep = " ", col.names = FALSE
  )
  return(file_path)
})

# Since the number of weight files (3) matches the number of GDS files (3),
# the `ldak_chrs` parameter is not required, but we include it for demonstration.
ldak_sites <- phenocause::sample_causal_sites.ldak(
  gds_paths = gds_paths,
  n_causal_sites = 500,
  ldak_ld_weights_paths = temp_ldak_paths,
  ldak_chrs = c(1, 2, 3),
  weights_power = -0.25, # Example to enrich for high-LD regions
  n_threads = 2
)

cat("Number of sites sampled with LDAK weights:", nrow(ldak_sites$causal_genotypes), "\n")
```

#### example 1.3: weighted sampling (custom weights)  

The `sample_causal_sites.custom()` mode offers maximum flexibility, allowing a researcher to test hypotheses about any user-defined genomic feature.  
For example, one could use weights based on functional annotation scores (e.g., CADD, PhyloP) to simulate a trait where causal variants are enriched in functionally important regions of the genome.  
The input file must contain `chr`, `pos`, and `weight` columns.  

```{r custom-weighted-sampling}
# Create temporary custom weight files with random weights for demonstration
tmp_custom_files <- sapply(1:3, function(chr_idx) {
  tmp_file <- tempfile(fileext = ".tsv")
  gds <- SeqArray::seqOpen(gds_paths[chr_idx])
  on.exit(SeqArray::seqClose(gds))
  
  annot_df <- data.frame(
      chr = SeqArray::seqGetData(gds, "chromosome"),
      pos = SeqArray::seqGetData(gds, "position"),
      rsid = SeqArray::seqGetData(gds, "annotation/id"),
      ref = SeqArray::seqGetData(gds, "$ref"),
      alt = SeqArray::seqGetData(gds, "$alt")
  )
  # For this example, we assign random weights
  annot_df$weight <- abs(rnorm(n = nrow(annot_df), mean = 0, sd = 3))
  data.table::fwrite(annot_df, file = tmp_file, sep = "\t")
  return(tmp_file)
})

custom_sites <- phenocause::sample_causal_sites.custom(
  gds_paths = gds_paths,
  n_causal_sites = 500,
  sampling_weights_paths = tmp_custom_files
)

cat("Number of sites sampled with custom weights:", nrow(custom_sites$causal_genotypes), "\n")
file.remove(tmp_custom_files)
```

#### example 1.4: collider sampling (visual check)  

To study collider bias, we need to generate sets of variants that are matched on allele frequency but differ in their correlation with population structure (as measured by PCs).  
The `sample_causal_sites.collider()` function does this by partitioning variants based on their squared multiple correlation ($R^2$) with a given set of PCs.  
It returns a "structured" set (high $R^2$) and a "non-structured" set (low $R^2$).  
We can visually confirm that the function works as intended by plotting the empirical cumulative distribution functions (ECDFs) of MAF and $R^2$ for both sets.  

```{r collider-sampling-basic}
# Load metadata to get the PC matrix
data(phenocause.metadata)
pca_matrix <- phenocause.metadata[, c("sample.id", paste0("PC", 1:10))]

# Run collider sampling
collider_sites <- phenocause::sample_causal_sites.collider(
  gds_paths = gds_paths,
  n_causal_sites = 50,
  pca_matrix = pca_matrix,
  n_threads = 2
)

# Combine annotations for plotting
structured_df <- collider_sites$annotations$causal_structured %>% dplyr::mutate(Set = "Structured")
non_structured_df <- collider_sites$annotations$causal_non_structured %>% dplyr::mutate(Set = "Non-Structured")
plot_df <- dplyr::bind_rows(structured_df, non_structured_df)

# Plot ECDF of R^2
p_r2 <- ggplot2::ggplot(plot_df, ggplot2::aes(x = r2, color = Set)) +
  ggplot2::stat_ecdf(geom = "step", size = 1.2) +
  ggplot2::labs(
    title = "ECDF of R-squared with PCs",
    x = "R-squared", y = "Cumulative Probability"
  ) +
  ggplot2::theme_classic() +
  ggplot2::theme(legend.position = "bottom")

# Plot ECDF of MAF
p_maf <- ggplot2::ggplot(plot_df, ggplot2::aes(x = maf, color = Set)) +
  ggplot2::stat_ecdf(geom = "step", size = 1.2) +
  ggplot2::labs(
    title = "ECDF of Minor Allele Frequency",
    x = "MAF", y = "Cumulative Probability"
  ) +
  ggplot2::theme_classic() +
  ggplot2::theme(legend.position = "bottom")

# Show plots side-by-side
print(p_r2 | p_maf)
```

The plots confirm that the two sets have highly distinct distributions of $R^2$ values, while their MAF distributions are nearly identical, just as required.  

---

### part 2: specifying the phenotype's causal components  

Once a set of causal genotypes has been sampled, the `simulate_phenotype()` function uses them to construct the final phenotype.  
This function operationalizes the theoretical models described earlier, allowing for the precise addition of genetic and confounding effects.  

#### example 2.1: basic quantitative trait  

The most fundamental simulation is of a trait with only a genetic component ($g$) and a residual error component ($E$).  
It is critical to note that even though the population is genetically structured, we are not simulating any *environmental* factor correlated with that structure.  
Therefore, as outlined in the motivation, the phenotypic covariance in this scenario can be fully modeled by a "deep GRM", and a mixed linear model including such a GRM but without PCs should be sufficient to control for test statistic inflation.  

```{r simulate-basic}
target_h2 <- 0.6

sim_basic <- phenocause::simulate_phenotype(
  causal_genotypes = causal_geno_df,
  heritability = target_h2
)

# Validation: Check if the observed heritability is close to the target
observed_h2 <- stats::var(sim_basic$phenotypes$g) / stats::var(sim_basic$phenotypes$y_quantitative)
cat(sprintf("Target h2: %.3f | Observed h2: %.3f\n", target_h2, observed_h2))

# Clarify function output structure
cat("\nStructure of the returned list:\n")
print(utils::str(sim_basic, max.level = 1))
cat("\nHead of the 'phenotypes' data frame:\n")
print(head(sim_basic$phenotypes))
```

#### example 2.2: adding a categorical confounder (ordinal scale)  

We can incorporate a categorical confounder on either a nominal or ordinal scale.  
The scale is determined by the class of the input vector: a `character` class is treated as nominal, while a `factor` is treated as ordinal, with effects that increase monotonically across the defined levels.  
This is useful for simulating variables like socioeconomic status.  
In this example, we create an ordinal confounder from the population labels by ordering them based on their mean value along PC1.  

```{r simulate-ordinal-confounder}
# Create an ordered factor where levels are sorted by their average PC1 value
data(phenocause.metadata)
confounder_levels <- phenocause.metadata %>%
  dplyr::group_by(population) %>%
  dplyr::summarise(mean_pc1 = mean(PC1)) %>%
  dplyr::arrange(mean_pc1) %>%
  dplyr::pull(population)

categorical_confounder <- factor(phenocause.metadata$population, levels = confounder_levels)
target_conf_var <- 0.1

sim_cat_conf <- phenocause::simulate_phenotype(
  causal_genotypes = causal_geno_df,
  heritability = 0.5,
  categorical_confounder = categorical_confounder,
  categorical_confounder_variance = target_conf_var
)

# Validation: Check observed variance and plot effects
p_categorical <- sim_cat_conf$phenotypes$p_categorical_confounder
observed_conf_var <- stats::var(p_categorical)
cat(sprintf("Target Confounder Variance: %.3f | Observed: %.3f\n", target_conf_var, observed_conf_var))

plot_df_ord <- data.frame(
  Confounder_Level = categorical_confounder,
  Confounding_Effect = p_categorical
)
ggplot2::ggplot(plot_df_ord, ggplot2::aes(x = Confounder_Level, y = Confounding_Effect)) +
  ggplot2::geom_boxplot() +
  ggplot2::labs(
    title = "Ordinal Confounder Effects",
    x = "Confounder Level (Ordered by Mean PC1)",
    y = "Simulated Confounding Effect (P_cc)"
  ) +
  ggplot2::theme_classic() +
  ggplot2::theme(axis.text.x = ggplot2::element_text(angle = 45, hjust = 1))
```

#### example 2.3: adding a confounder correlated with ancestry  

This simulates the classic scenario where a non-genetic factor is correlated with genetic ancestry and also affects the phenotype.  
In this case, the confounding is external to the genetic covariance structure itself.  
Therefore, a GRM alone is not expected to be sufficient to control inflation in a GWAS, and additional covariates like PCs may be necessary.  
Here, we use PC1 as our ancestry component, $Q$.  

```{r simulate-ancestry-confounder}
set.seed(789)
data(phenocause.metadata)
pc1 <- phenocause.metadata$PC1

# Define simulation parameters
target_anc_cor <- 0.8 
target_anc_rel_var <- 0.15 

sim_anc_conf <- phenocause::simulate_phenotype(
  causal_genotypes = causal_geno_df,
  heritability = 0.5,
  ancestry_component = pc1,
  ancestry_confounder_coeff = 0.6,
  ancestry_confounder_cor = target_anc_cor,
  ancestry_confounder_rel_var = target_anc_rel_var
)

# Validation
g_anc <- sim_anc_conf$phenotypes$g
p_ancestry <- sim_anc_conf$phenotypes$p_ancestry_confounder
x_ancestry <- sim_anc_conf$confounders$X_ancestry_confounder
observed_anc_cor <- stats::cor(pc1, x_ancestry)
observed_anc_rel_var <- stats::var(p_ancestry) / stats::var(g_anc)
cat(sprintf("Target Cor(Q, X_ac): %.3f | Observed: %.3f\n", target_anc_cor, observed_anc_cor))
cat(sprintf("Target Var(P_ac)/Var(g): %.3f | Observed: %.3f\n", target_anc_rel_var, observed_anc_rel_var))
```

#### example 2.4: adding a confounder correlated with the polygenic score  

This models a distinct form of confounding, such as pleiotropy, where the confounding variable is correlated with an individual's true polygenic score ($g$) itself.  

```{r simulate-genetic-confounder}
set.seed(456)
target_g_cor <- 0.7 
target_g_rel_var <- 0.2

sim_genetic_conf <- phenocause::simulate_phenotype(
  causal_genotypes = causal_geno_df,
  heritability = 0.5,
  genetic_confounder_coeff = 0.5,
  genetic_confounder_cor = target_g_cor,
  genetic_confounder_rel_var = target_g_rel_var
)

# Validation
g <- sim_genetic_conf$phenotypes$g
p_genetic <- sim_genetic_conf$phenotypes$p_genetic_confounder
x_genetic <- sim_genetic_conf$confounders$X_genetic_confounder
observed_g_cor <- stats::cor(g, x_genetic)
observed_g_rel_var <- stats::var(p_genetic) / stats::var(g)
cat(sprintf("Target Cor(g, X_gc): %.3f | Observed: %.3f\n", target_g_cor, observed_g_cor))
cat(sprintf("Target Var(P_gc)/Var(g): %.3f | Observed: %.3f\n", target_g_rel_var, observed_g_rel_var))
```

---


### part 3: full simulation demonstrating collider bias  

Finally, we present a complete simulation and analysis pipeline to demonstrate collider bias rigorously.  
To do this, we must compare results across a full 2x2 factorial experiment.  
This allows us to distinguish true collider bias from classic confounding by population structure.  

The experiment is defined by two factors:  
1. **causal architecture**: Is the phenotype generated from **structured** or **non-structured** causal variants?  
2. **gwas model**: Does the association model adjust for **PCs** or not?  

This design creates four distinct scenarios, each with a specific expected outcome for the test statistics of non-causal variants:  
* **scenario 1 (structured causal, with pcs)**: The condition for **collider bias**. We expect inflation (Type I Error > 0.05, Î» > 1) *only* in the non-causal *structured* test set.  
* **scenario 2 (structured causal, no pcs)**: The condition for **confounding by population structure**. We expect inflation in *both* non-causal test sets.  
* **scenario 3 (non-structured causal, with pcs)**: A crucial **negative control**. We expect no inflation in either test set.  
* **scenario 4 (non-structured causal, no pcs)**: Another **negative control**. We expect no inflation in either test set.  

Only by observing the specific inflation pattern in Scenario 1, and the lack thereof in the control scenarios, can we confidently identify collider bias.  

#### step 3.1: setup - sampling all variant sets  

First, we run the collider sampling function once to generate all the variant sets we will need for the four experimental scenarios.  
This gives us a consistent set of causal and non-causal variants for the entire experiment.  

```{r collider-gwas-setup-final}

set.seed(2024)
n_replicates <- 3

# Load metadata and select the PCs to be used for both sampling and GWAS adjustment
data(phenocause.metadata)
pca_matrix_collider <- phenocause.metadata[, c("sample.id", paste0("PC", 1:10))]

# Sample the four key variant sets
collider_sets <- phenocause::sample_causal_sites.collider(
  gds_paths = gds_paths,
  n_causal_sites = 100,
  mac_threshold = 20,
  pca_matrix = pca_matrix_collider,
  n_threads = 2
)

# Causal sets for phenotype simulation:
causal_geno_structured <- collider_sets$genotypes$causal_structured
causal_geno_nonstructured <- collider_sets$genotypes$causal_non_structured

# Non-causal sets for GWAS testing:
test_variants_structured <- collider_sets$annotations$non_causal_structured$variant.id.internal
test_variants_nonstructured <- collider_sets$annotations$non_causal_non_structured$variant.id.internal
```

#### step 3.2: the main simulation and gwas loop  

We now loop through all replicates and all four scenarios.  
For maximum clarity, we will use standard `for` loops.  
Inside the loops, we will simulate the appropriate phenotype, fit the correct null model, run association tests, and collect all raw results into a single data frame.  

```{r full-collider-gwas-loop-final}

cat(glue::glue("Running {n_replicates} replicates across 4 scenarios..."),"\n")
all_gwas_results_list <- list()

# Define the 4 scenarios
scenarios <- tidyr::expand_grid(
  causal_set_name = c("structured", "nonstructured"),
  gwas_model_name = c("with_pcs", "no_pcs")
)

# Main loop over replicates
for (i in 1:n_replicates) {
  cat(glue::glue("Starting replicate {i}..."), "\n")
  
  # Loop over the 4 scenarios
  for (j in 1:nrow(scenarios)) {
    scenario <- scenarios[j, ]
    
    # Select the correct causal genotype set for the current scenario
    current_causal_geno <- if (scenario$causal_set_name == "structured") {
      causal_geno_structured
    } else {
      causal_geno_nonstructured
    }
    
    # Simulate phenotype
    sim_pheno <- phenocause::simulate_phenotype(
      causal_genotypes = current_causal_geno,
      n_annot_cols = 8, # Fix: specify correct number of annotation columns
      heritability = 0.5
    )
    
    # Prepare data for GENESIS
    covar_names <- paste0("PC", 1:10)
    pheno_df <- phenocause.metadata %>%
      dplyr::select(sample.id, dplyr::all_of(covar_names)) %>%
      dplyr::inner_join(sim_pheno$phenotypes, by = "sample.id")

    # Fix: Convert to data.frame and filter for complete cases
    pheno_df_complete <- as.data.frame(pheno_df)
    complete_samples <- stats::complete.cases(pheno_df_complete[, c("y_quantitative", covar_names)])
    pheno_df_complete <- pheno_df_complete[complete_samples, ]
    
    # Fix: Create AnnotatedDataFrame in steps
    annodf <- Biobase::AnnotatedDataFrame()
    Biobase::pData(annodf) <- pheno_df_complete
    metadata <- data.frame(labelDescription = colnames(pheno_df_complete))
    Biobase::varMetadata(annodf) <- metadata

    # Select covariates based on the current scenario
    current_covars <- if (scenario$gwas_model_name == "with_pcs") {
      covar_names
    } else {
      NULL # No covariates for the "no_pcs" model
    }
    
    # Fit null model
    nullmod <- GENESIS::fitNullModel(
      annodf, outcome = "y_quantitative",
      covars = current_covars,
      family = "gaussian",
      verbose = FALSE
    )
    
    # Helper function to run GWAS on a set of variant IDs
    run_gwas <- function(gds_paths, variant_ids, null_model) {
      gds <- SeqArray::seqOpen(gds_paths[1])
      on.exit(SeqArray::seqClose(gds))
      SeqArray::seqSetFilter(gds, sample.id = pheno_df_complete$sample.id, variant.id = variant_ids, verbose = FALSE)
      if (length(SeqArray::seqGetData(gds, "variant.id")) == 0) return(NULL)
      seqData <- SeqVarTools::SeqVarData(gds)
      iterator <- SeqVarTools::SeqVarBlockIterator(seqData, verbose = FALSE)
      assoc_df <- GENESIS::assocTestSingle(iterator, null.model = null_model, verbose = FALSE)
      return(assoc_df)
    }
    
    # Run GWAS on both non-causal test sets
    gwas_structured <- run_gwas(gds_paths, test_variants_structured, nullmod)
    gwas_nonstructured <- run_gwas(gds_paths, test_variants_nonstructured, nullmod)
    
    # Add identifiers and store results
    if (!is.null(gwas_structured)) gwas_structured$test_set <- "structured"
    if (!is.null(gwas_nonstructured)) gwas_nonstructured$test_set <- "nonstructured"
    
    replicate_results <- dplyr::bind_rows(gwas_structured, gwas_nonstructured)
    
    if (nrow(replicate_results) > 0) {
      replicate_results$replicate <- i
      replicate_results$causal_set_name <- scenario$causal_set_name
      replicate_results$gwas_model_name <- scenario$gwas_model_name
      all_gwas_results_list[[length(all_gwas_results_list) + 1]] <- replicate_results
    }
  }
}

# Combine all results
all_gwas_results <- dplyr::bind_rows(all_gwas_results_list)
```

#### step 3.3: calculating summary statistics  

With the raw results from all replicates and scenarios collected, we can now calculate our summary metrics.  
For each of the experimental conditions, we compute the Type I error rate and the genomic inflation factor, lambda.  

```{r collider-gwas-summarize-final}
summary_stats <- all_gwas_results %>%
  dplyr::group_by(replicate, causal_set_name, gwas_model_name, test_set) %>%
  dplyr::summarise(
    type_I_error = mean(Score.pval <= 0.05, na.rm = TRUE),
    lambda = median(Score.Stat^2, na.rm = TRUE) / stats::qchisq(0.5, df = 1),
    .groups = "drop"
  )

cat("Summary of results across all conditions and replicates:\n")
print(summary(summary_stats))

summary_stats %>% 
      dplyr::group_by(test_set, gwas_model_name) %>% 
      dplyr::summarise(mean_lambda = mean(lambda),
                          mean_type1_error_rate = mean(type_I_error))

```

#### step 3.4: visualization and interpretation  

The final step is to plot the distributions of the type-I error rate and Genomic Control Lambda parameter. The results of the models with and without principal components had a very different range of values in the Y axis. Thus, we present each group of results separately to avoid losing resolution.


```{r collider-make-plots, results='hold', fig.width=7, fig.height=10}
library(viridis)

# Prepare data for plotting
summary_stats$causal_set_name <- factor(
  summary_stats$causal_set_name, 
  levels = c("structured", "nonstructured"),
  labels = c("Causal Set: Structured", "Causal Set: Non-Structured")
)
summary_stats$gwas_model_name <- factor(
  summary_stats$gwas_model_name, 
  levels = c("with_pcs", "no_pcs"),
  labels = c("GWAS Model: With PCs", "GWAS Model: No PCs")
)

# Helper function to prepare plots
make_panels <- function(df, y_var, y_label, intercept){
                  df %>% 
                  ggplot(aes(x = test_set, y = .data[[y_var]], fill = test_set)) +
                  geom_boxplot() +
                  geom_hline(yintercept = intercept, linetype = "dashed", color = "red") +
                  facet_grid("causal_set_name", scales = "free") +
                  scale_x_discrete(labels = c("nonstructured" = "Non-Struct.", "structured" = "Struct.")) +
                  labs(
                    x = "Set of Non-Causal Variants Tested",
                    y = y_label
                  ) +
                  theme_classic() +
                  theme(legend.position = "none")
}



p_lambda_with_pcs <- summary_stats %>% 
  dplyr::filter(gwas_model_name=="GWAS Model: With PCs") %>% 
  make_panels(y_var = "lambda", y_label = "Genomic Inflation Factor (Lambda)", intercept = 1)

p_error_rate_with_pcs <- summary_stats %>% 
    dplyr::filter(gwas_model_name=="GWAS Model: With PCs") %>% 
    make_panels(y_var = "type_I_error", y_label="Type-I error rate", intercept = 0.05)

p_lambda_without_pcs <- summary_stats %>% 
  dplyr::filter(gwas_model_name=="GWAS Model: No PCs") %>% 
  make_panels(y_var = "lambda", y_label = "Genomic Inflation Factor (Lambda)", intercept = 1)

p_error_rate_without_pcs <- summary_stats %>% 
    dplyr::filter(gwas_model_name=="GWAS Model: No PCs") %>% 
    make_panels(y_var = "type_I_error", y_label="Type-I error rate", intercept = 0.05)

```


We show first the results of the models that did not include principal components.  

```{r collider-gwas-plot-no-pcs, results='hold', fig.width=9, fig.height=5}

merged_plot_without_pcs <- ( p_lambda_without_pcs + p_error_rate_without_pcs )  +
              patchwork::plot_annotation(
                    title = "GWAS models without using principal components",
                    subtitle = paste0(glue::glue("{n_replicates} Replicates.")))

print(merged_plot_without_pcs)
```
 
As expected, not including PCs in the GWAS model leads to an increase in the type-I error rate and the genomic inflation inflation factor across al conditions tested. However, these increases are much larger in the scenarios where the causal sites are strongly correlated with principal components. Importantly, the inflation is much larger among non-causal sites that are correlated with population structure than in non-causal sites not correlated with population structure, regardless of whether the causal sites are correlated or not with population structure. Notice that in these analyses there is no collider bias because PCs were not used to adjust the models. This observation hints that attributing inflation specifically to collider bias (i.e. by introducing PCs) is difficult, because even in the models without collider bias (i.e. without using PCs as covariates), the inflation among non-causal sites correlated with population structure is higher than the inflation of non-causal sites non-correlated with population structure. Furthermore, even if the PCs do induce collider bias, they are correlated with population structure and are thus expected to at least partially correct for the genetic covariance induce by population structure. We can confirm this prediciton by plotting the results of the models that included PCs.  

```{r collider-gwas-plot-with-pcs, results='hold', fig.width=9, fig.height=5}
merged_plot_with_pcs <- ( p_lambda_with_pcs + p_error_rate_with_pcs )  +
              patchwork::plot_annotation(
                    title = "GWAS models adjusted with principal components",
                    subtitle = paste0(glue::glue("{n_replicates} Replicates.")))

print(merged_plot_with_pcs)
```

We observe that including PCs did reduce the type-I errpr rate and genomic inflation factor in all scenarios, but this effect was much more pronounced among non-causal sites correlated with population structure. Furthermore, including PCs in the model decreased the type-I error rate and genomic inflation factor of structured non-causal sites to values lower than those of the non-causal non-structured sites. However, when the causal sites are correlated with population structure, the non-causal structured sites have a higher inflation than the non-causal non-structured sites even when PCs are included in the model. All in all, these results show that when PCs are included in the model, the inflation is higher among the non-causal sites that have the same degree of association (which could be either low or high) with population structure as the causal sites.   


